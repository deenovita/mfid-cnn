{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "On59xUm17Y9ykT0xyMuqQ6",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import numpy\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from numpy import exp\n",
    "from numpy import sum\n",
    "from numpy import log\n",
    "from numpy import zeros\n",
    "from numpy import float64\n",
    "from numpy import int64\n",
    "from numpy import unravel_index\n",
    "from numpy import max\n",
    "from numpy import argmax\n",
    "from numpy import copy\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from numpy import array\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "hzLFVr0Oraw1j63toVb663",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(outputs):\n",
    "    \"\"\"\n",
    "    Fungsi cross entropy untuk mendapatkan nilai loss dari nilai hasil deteksi.\n",
    "\n",
    "    :param outputs: nilai hasil deteksi.\n",
    "    :return: nilai loss hasil deteksi.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_value = -log(outputs)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def relu_activation(summa_nrc):\n",
    "    \"\"\"\n",
    "    Fungsi aktivasi Rectified Linear Unit (ReLU) untuk melakukan aktifasi pada nilai input.\n",
    "\n",
    "    :param summa_nrc: hasil operasi konvolusi untuk diaktifkan.\n",
    "    :return: nilai hasil aktivasi ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    if summa_nrc > 0:\n",
    "        return summa_nrc\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def derivative_relu(summa_nrc):\n",
    "    \"\"\"\n",
    "    Untuk mencari nilai turunan dari fungsi aktivasi ReLU.\n",
    "\n",
    "    :param summa_nrc: nilai hasil aktivasi ReLU.\n",
    "    :return: nilai hasil turunan ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    if summa_nrc > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def softmax_activation(summa):\n",
    "    \"\"\"\n",
    "    Fungsi aktivasi softmax untuk melakukan aktivasi nilai dari fully connected layer ke dalam bentuk probabilitas.\n",
    "\n",
    "    :param summa: nilai pembobotan di fully connected layer.\n",
    "    :return: nilai hasil aktivasi dalam bentuk probabilitas.\n",
    "    \"\"\"\n",
    "\n",
    "    numerator = exp(summa)\n",
    "    denominator = sum(numerator)\n",
    "    softmax_value = numerator / denominator\n",
    "\n",
    "    return softmax_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IcxVMNo8iM6sI62JLpJ44H",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    \"\"\"\n",
    "    Kelas untuk membuat confusion matrix dari performa model.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    actual: \n",
    "        label/kelas sebenarnya dari citra.\n",
    "    result: \n",
    "        label/kelas citra hasil deteksi.\n",
    "    classes: \n",
    "        representasi kelas output.\n",
    "    w_mask:\n",
    "        representasi kelas tidak bermasker(0).\n",
    "    mask:\n",
    "        representasi kelas bermasker(1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actual_class, result_class):\n",
    "\n",
    "        self.actual = actual_class  # Dalam bentuk array 1D\n",
    "        self.result = result_class  # Dalam bentuk array 1D\n",
    "        \n",
    "        # Classes: 0 (Tidak bermasker), 1 (bermasker)\n",
    "        self.classes = [0, 1]\n",
    "        self.w_mask = self.classes[0]\n",
    "        self.mask = self.classes[1]\n",
    "\n",
    "    def true_negative(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai true negative.\n",
    "\n",
    "        :returns: nilai true negative.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "\n",
    "        # Nilai awal true negative (tn)\n",
    "        tn = 0\n",
    "\n",
    "        for act_, res_ in zip(actual, result):\n",
    "            if act_ == self.w_mask and res_ == self.w_mask:\n",
    "                tn += 1\n",
    "            else: pass\n",
    "\n",
    "        return tn\n",
    "\n",
    "    def false_positive(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai false positive.\n",
    "\n",
    "        :returns: nilai false positive.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "        fp = 0\n",
    "\n",
    "        for act_, res_ in zip(actual, result):\n",
    "            if act_ == self.w_mask and res_ == self.mask:\n",
    "                fp += 1\n",
    "            else: pass\n",
    "\n",
    "        return fp\n",
    "\n",
    "    def false_negative(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai false negative.\n",
    "\n",
    "        :returns: nilai false negative.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "        fn = 0\n",
    "\n",
    "        for act_, res_ in zip(actual, result):\n",
    "            if act_ == self.mask and res_ ==self.w_mask:\n",
    "                fn += 1\n",
    "            else: pass\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def true_positive(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai true positive.\n",
    "\n",
    "        :returns: nilai true positive.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "        tp = 0\n",
    "\n",
    "        for act_, res_ in zip(actual, result):\n",
    "            if act_ == self.mask and res_ == self.mask:\n",
    "                tp += 1\n",
    "            else: pass\n",
    "\n",
    "        return tp\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai recall.\n",
    "\n",
    "        :returns: nilai recall.\n",
    "        \"\"\"\n",
    "        tp = self.true_positive()\n",
    "        fn = self.false_negative()\n",
    "\n",
    "        recall = tp / (fn + tp)\n",
    "\n",
    "        return recall\n",
    "\n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai recall.\n",
    "\n",
    "        :returns: nilai recall.\n",
    "        \"\"\"\n",
    "        tp = self.true_positive()\n",
    "        fp = self.false_positive()\n",
    "\n",
    "        precision = tp / (fp + tp)\n",
    "\n",
    "        return precision\n",
    "\n",
    "    def f1_score(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai f1-score.\n",
    "\n",
    "        :returns: nilai f1-score.\n",
    "        \"\"\"\n",
    "\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        return f1_score\n",
    "\n",
    "    def display_table(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menampilkan visualisasi tabel confusion matrix.\n",
    "\n",
    "        :returns: tabel confusion matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        tn = self.true_negative()\n",
    "        fp = self.false_positive()\n",
    "        fn = self.false_negative()\n",
    "        tp = self.true_positive()\n",
    "\n",
    "        header_table = [\"Actual/Result\", \"0\", \"1\"]\n",
    "        table = [[\"0\", tn, fp], [\"1\", fn, tp]]\n",
    "        tformat = \"grid\"\n",
    "\n",
    "        return print(tabulate(tabular_data=table, headers=header_table,tablefmt=tformat))\n",
    "\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menghitung nilai accuracy.\n",
    "\n",
    "        :returns: nilai accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "\n",
    "        acc_ = 0\n",
    "        n_data = len(result)\n",
    "\n",
    "        for act_, res_ in zip(actual, result):\n",
    "            if act_ == res_:\n",
    "                acc_ += 1\n",
    "            else:\n",
    "                acc_ += 0\n",
    "\n",
    "        accuracy = acc_ / n_data\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def model_performance_graph(self):\n",
    "        \"\"\"\n",
    "        Fungsi untuk menampilkan performa model dalam graph bar.\n",
    "\n",
    "        :returns: graph bar.\n",
    "        \"\"\"\n",
    "\n",
    "        actual = self.actual\n",
    "        result = self.result\n",
    "\n",
    "        accuracy = self.accuracy()\n",
    "        recall = self.recall()\n",
    "        precision = self.precision()\n",
    "        f1_score = self.f1_score()\n",
    "\n",
    "        data = {'accuracy':accuracy, 'racall':recall, 'precision': precision, 'f1_score':f1_score}\n",
    "\n",
    "        performance = list(data.keys())\n",
    "        values = list(data.values())\n",
    "\n",
    "        # creating the bar plot\n",
    "        plot.figure(figsize = (10, 5))\n",
    "        plot.bar(performance, values, color ='gray',width = 0.4)\n",
    "        plot.show()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IqAWGexiH1vWW5X3sMYXIv",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    Kelas untuk membuat convolution layer pertama.\n",
    "    Satu citra input, dengan kernel 3 dimensi (n_kernel, kernel_size, kernel_size).\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    kernel: n-dimensional array.\n",
    "        kernel yang digunakan pada layer ini.\n",
    "    bias: 1-dimensional array.\n",
    "        bias yang digunakan untuk layer ini.\n",
    "    padding: int.\n",
    "        jumlah padding yang digunakan di layer ini, nilai default=0.\n",
    "    stride: int.\n",
    "        jumlah perpindahan piksel untuk melakukan uperasi konvolusi, nilai default=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, bias, padding = 0, stride = 1):\n",
    "        self.kernel = kernel\n",
    "        self.bias = bias\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forwardpass(self, image):\n",
    "        \"\"\"\n",
    "        Fungsi untuk convolution layer ini melakukan forwardpass.\n",
    "\n",
    "        :param image: citra input 2-dimensi (width, height).\n",
    "        :return: hasil operasi konvolusi yang telah aktivasi relu, berbentuk n-dimensional array.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dapatkan jumlah dan ukuran kernel yang digunakan\n",
    "        n_kernel, kernel_size, _ = self.kernel.shape\n",
    "\n",
    "        # Menghitung ukuran output salah satu sisi (width dan height berukuran sama)\n",
    "        cl_size = int((image.shape[0] - kernel_size + 2 * self.padding) / self.stride) + 1\n",
    "\n",
    "        # Inisialisasi cl dengan dimensinya, nilai 0 untuk seluruh piksel\n",
    "        cl = zeros((n_kernel, cl_size, cl_size), dtype=float64)\n",
    "\n",
    "        # Inisialisasi output turunan fungsi relu, nilai 0 seluruh piksel\n",
    "        d_cl_summa = zeros(cl.shape, dtype=float64)\n",
    "\n",
    "        for nk in range(n_kernel):\n",
    "            for row in range(cl_size):\n",
    "                for col in range(cl_size):\n",
    "                    # Mendapatkan lokasi region untuk operasi konvolusi\n",
    "                    local_region = image[row:(row + kernel_size), col:(col + kernel_size)]\n",
    "\n",
    "                    # Operasi konvolusi\n",
    "                    summa_nrc = sum(local_region * self.kernel[nk]) + self.bias[nk]\n",
    "\n",
    "                    # Operasi aktivasi relu\n",
    "                    cl[nk, row, col] = relu_activation(summa_nrc)\n",
    "\n",
    "                    # Operasi turunan relu\n",
    "                    d_cl_summa[nk, row, col] = derivative_relu(cl[nk, row, col])\n",
    "\n",
    "        return cl, d_cl_summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "6Aq12bRo8gINWWDUWznrlr",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class Conv3D:\n",
    "    \"\"\"\n",
    "    Kelas untuk membuat convolution layer kedua dan seterusnya.\n",
    "    Dua atau lebih citra input, dengan kernel 3 dimensi (n_kernel, kernel_size, kernel_size).\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    kernel: n-dimensional array.\n",
    "        Kernel yang digunakan pada layer ini.\n",
    "    bias: 1-dimensional array.\n",
    "        bias yang digunakan untuk layer ini.\n",
    "    padding: int.\n",
    "        jumlah padding yang digunakan di layer ini, nilai default=0.\n",
    "    stride: int.\n",
    "        jumlah perpindahan piksel untuk melakukan uperasi konvolusi, nilai default=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel, bias, padding=0, stride=1):\n",
    "        self.kernel = kernel\n",
    "        self.bias = bias\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forwardpass(self, pl):\n",
    "        \"\"\"\n",
    "        Fungsi untuk convolutional layer ini melakukan forwardpass.\n",
    "\n",
    "        :param pl: Hasil operasi pooling sebelumnya.\n",
    "        :return: Hasil operasi konvolusi yang telah diaktifasi relu, berbentuk n-dimensional array.\n",
    "        \"\"\"\n",
    "\n",
    "        # Mendapatkan jumlah kernel sekarang, sebelumnya, dan ukuran kernel dari dimensi kernel yang digunakan saat ini\n",
    "        n_kernel_current, n_kernel_prev, kernel_size, _ = self.kernel.shape\n",
    "\n",
    "        # Menghitung ukuran output salah satu sisi (width dan height berukuran sama)\n",
    "        cl_size = int((pl.shape[1] - kernel_size + 2 * self.padding) / self.stride) + 1\n",
    "\n",
    "        # Inisialisasi cl dan memberikan nilai 0 untuk seluruh piksel\n",
    "        cl = zeros((n_kernel_current, cl_size, cl_size), dtype=float64)\n",
    "\n",
    "        # Inisialisasi output turunan aktivasi relu dan memberikan nilai 0\n",
    "        d_cl_summa = zeros(cl.shape, dtype=float64)\n",
    "\n",
    "        d_summa_pl = zeros(pl.shape + cl.shape, dtype=float64)\n",
    "\n",
    "        for nkc in range(n_kernel_current):\n",
    "            for row in range(cl_size):\n",
    "                for col in range(cl_size):\n",
    "                    # Mendapatkan lokasi region untuk operasi konvolusi\n",
    "                    local_region = pl[0:n_kernel_prev, row:(row + kernel_size), col:(col + kernel_size)]\n",
    "                    # Operasi konvolusi\n",
    "                    summa_nrc = sum(local_region * self.kernel[nkc]) + self.bias[nkc]\n",
    "                    # Operasi aktivasi\n",
    "                    cl[nkc, row, col] = relu_activation(summa_nrc)\n",
    "                    # Operasi turunan aktivasi\n",
    "                    d_cl_summa[nkc, row, col] = derivative_relu(cl[nkc, row, col])\n",
    "                    d_summa_pl[0:n_kernel_prev, row:(row + kernel_size), col:(col + kernel_size),nkc ,row, col] = self.kernel[nkc]\n",
    "\n",
    "        return cl, d_cl_summa, d_summa_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XZQMloM58y7n6In2BrEj3s",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    \"\"\"\n",
    "    Kelas untuk membuat pooling layer dengan operasi max pooling. Jumlah citra input dan output sama.\n",
    "\n",
    "    Attribute:\n",
    "\n",
    "    pool_size: int\n",
    "        Ukuran pooling yang digunakan, bernilai default=2 yang berarti array 2 x 2.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size=2):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forwardpass(self, cl):\n",
    "        \"\"\"\n",
    "        Fungsi untuk pooling layer melakukan forwardpass.\n",
    "\n",
    "        :param cl: Hasil convolutional layer sebelumnya.\n",
    "        :return: Hasil operasi pooling dan lokasi indeks dari operasi pooling.\n",
    "        \"\"\"\n",
    "\n",
    "        # Mendapatkan jumlah kernel dan ukuran kernel dari dimensi output conv layer sebelumnya\n",
    "        n_kernel, cl_size, _ = cl.shape\n",
    "        pl_size = int(cl_size / self.pool_size)\n",
    "\n",
    "        pl = zeros((n_kernel, pl_size, pl_size), dtype=float64)\n",
    "        index_loc = zeros((n_kernel, pl_size, pl_size), dtype=(int64, 2))\n",
    "\n",
    "        for nk in range(n_kernel):\n",
    "            for row in range(pl_size):\n",
    "                for col in range(pl_size):\n",
    "                    local_region = cl[nk,(2 * row):(2 * row + 2),(2 * col):(2 * col + 2)]\n",
    "                    pl[nk, row, col] = max(local_region)\n",
    "                    local_index = unravel_index(argmax(local_region), local_region.shape)\n",
    "                    index_loc[nk, row, col] = [2 * row + local_index[0], 2 * col + local_index[1]]\n",
    "\n",
    "        return pl, index_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "iSlhf5ohLWPiXLLxz7bAtU",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    \"\"\"\n",
    "    Kelas untuk membuat arsitektur ConvNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_kernel_1, n_kernel_2, n_kernel_3):\n",
    "        # Ukuran kernel (width dan height sama, diambil salah satu\n",
    "        KERNEL_SIZE = 5\n",
    "        # Batas minimum\n",
    "        MIN = -0.1\n",
    "        # Batas maksimum\n",
    "        MAX = 0.1\n",
    "        # Banyak piksel (pl width x pl height) setelah flatten di fully connected layer\n",
    "        FLATTEN_PIXELS = 16\n",
    "        N_OUTPUTS = 2\n",
    "\n",
    "        # Inisialisasi kernel dan bias convolution layer 1\n",
    "        self.kernel_1 = random.uniform(low=MIN, high=MAX, size=(n_kernel_1, KERNEL_SIZE, KERNEL_SIZE))\n",
    "        self.bias_1 = random.uniform(low=MIN, high=MAX, size=n_kernel_1)\n",
    "\n",
    "        # Inisialisasi kernel dan bias convolution layer 2\n",
    "        self.kernel_2 = random.uniform(low=MIN, high=MAX, size=(n_kernel_2, n_kernel_1, KERNEL_SIZE, KERNEL_SIZE))\n",
    "        self.bias_2 = random.uniform(low=MIN, high=MAX, size=n_kernel_2)\n",
    "\n",
    "        # Inisialisasi kernel dan bias convolution layer 3\n",
    "        self.kernel_3 = random.uniform(low=MIN, high=MAX, size=(n_kernel_3, n_kernel_2, KERNEL_SIZE, KERNEL_SIZE))\n",
    "        self.bias_3 = random.uniform(low=MIN, high=MAX, size=n_kernel_3)\n",
    "\n",
    "        # Inisialisasi weight(bobot) dan bias fully connected layer\n",
    "        self.weight = random.uniform(low=MIN, high=MAX, size=(N_OUTPUTS, n_kernel_3 * FLATTEN_PIXELS))\n",
    "        self.bias = random.uniform(low=MIN, high=MAX, size=N_OUTPUTS)\n",
    "\n",
    "    def forwardpass(self, image):\n",
    "        \"\"\"\n",
    "        Fungsi untuk melakukan forwardpass.\n",
    "\n",
    "        :param image: citra input.\n",
    "        :return: citra dengan n-dimesional.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Convolution layer 1\n",
    "        conv_layer_1 = Conv2D(self.kernel_1, self.bias_1, padding=0, stride=1)\n",
    "        cl1, d_cl1_sum1 = conv_layer_1.forwardpass(image)\n",
    "\n",
    "        # Pooling layer 1\n",
    "        pool_layer_1 = MaxPool(pool_size=2)\n",
    "        pl1, index_loc1 = pool_layer_1.forwardpass(cl1)\n",
    "\n",
    "        # Convolution layer 2\n",
    "        conv_layer_2 = Conv3D(self.kernel_2, self.bias_2, padding=0, stride=1)\n",
    "        cl2, d_cl2_sum2, d_sum2_pl1 = conv_layer_2.forwardpass(pl1)\n",
    "\n",
    "        # Pooling layer 2\n",
    "        pool_layer_2 = MaxPool(pool_size=2)\n",
    "        pl2, index_loc2 = pool_layer_2.forwardpass(cl2)\n",
    "\n",
    "        # Convolution layer 3\n",
    "        conv_layer_3 = Conv3D(self.kernel_3, self.bias_3, padding=0, stride=1)\n",
    "        cl3, d_cl3_sum3, d_sum3_pl2 = conv_layer_3.forwardpass(pl2)\n",
    "\n",
    "        # Pooling layer 3\n",
    "        pool_layer_3 = MaxPool(pool_size=2)\n",
    "        pl3, index_loc3 = pool_layer_3.forwardpass(cl3)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        # Merubah jadi 1 dimensi (n, dim1, dim2) -> n images (dim1)\n",
    "        flattens = pl3.flatten()\n",
    "        # Penjumlahah dari perkalian pembobotan\n",
    "        summa = dot(self.weight, flattens) + self.bias\n",
    "        # Class 0 dan 1 , outputs [0, 1]\n",
    "        outputs = softmax_activation(summa)\n",
    "\n",
    "        return flattens, outputs\n",
    "\n",
    "    def training(self, images_training, labels_training, n_epoch, learn_rate):\n",
    "        \"\"\"\n",
    "        Fungsi untuk melakukan pelatihan data.\n",
    "\n",
    "        :param images_training: data citra latih.\n",
    "        :param labels_training: data label dari citra latih.\n",
    "        :param n_epoch: jumlah epoch.\n",
    "        :param learn_rate: nilai learning rate.\n",
    "        :return: epoch, rata-rata loss, dan akurasi.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n_kernel_1 = self.kernel_1.shape[0]\n",
    "        n_kernel_2 = self.kernel_2.shape[0]\n",
    "        n_kernel_3 = self.kernel_3.shape[0]\n",
    "\n",
    "        train_epochs = []\n",
    "        train_average_losses = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        # Permutation - copy then shuffle\n",
    "        len_perm = random.permutation(len(images_training))\n",
    "        images = images_training[len_perm]\n",
    "        labels = labels_training[len_perm]\n",
    "\n",
    "        print(\"Pelatihan data: \")\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "\n",
    "            train_losses = []\n",
    "            correct_labels = 0\n",
    "\n",
    "            progressbar_image = tqdm(total=len(images))\n",
    "\n",
    "            for image, label in zip(images, labels):\n",
    "                # Convolution layer 1\n",
    "                conv_layer_1 = Conv2D(self.kernel_1, self.bias_1, padding=0, stride=1)\n",
    "                cl1, d_cl1_sum1 = conv_layer_1.forwardpass(image)\n",
    "\n",
    "                # Polling layer 1\n",
    "                pool_layer_1 = MaxPool(pool_size=2)\n",
    "                pl1, index_loc1 = pool_layer_1.forwardpass(cl1)\n",
    "\n",
    "                # Convolution layer 2\n",
    "                conv_layer_2 = Conv3D(self.kernel_2, self.bias_2, padding=0, stride=1)\n",
    "                cl2, d_cl2_sum2, d_sum2_pl1 = conv_layer_2.forwardpass(pl1)\n",
    "\n",
    "                # Pooling layer 2\n",
    "                pool_layer_2 = MaxPool(pool_size=2)\n",
    "                pl2, index_loc2 = pool_layer_2.forwardpass(cl2)\n",
    "\n",
    "                # Convolution layer 3\n",
    "                conv_layer_3 = Conv3D(self.kernel_3, self.bias_3, padding=0, stride=1)\n",
    "                cl3, d_cl3_sum3, d_sum3_pl2 = conv_layer_3.forwardpass(pl2)\n",
    "\n",
    "                # Pooling layer 3\n",
    "                pool_layer_3 = MaxPool(pool_size=2)\n",
    "                pl3, index_loc3 = pool_layer_3.forwardpass(cl3)\n",
    "\n",
    "                # Fully connected layer\n",
    "                flattens = pl3.flatten()\n",
    "                weighted = dot(self.weight, flattens) + self.bias\n",
    "                outputs = softmax_activation(weighted)\n",
    "\n",
    "                train_losses.append(cross_entropy(outputs[label]))\n",
    "                correct_labels += 1 if argmax(outputs) == label else 0\n",
    "\n",
    "                # Backpropagation\n",
    "                # Gradient softmax - fc layer\n",
    "                dl_sum = copy(outputs)\n",
    "                dl_sum[label] = outputs[label] - 1\n",
    "\n",
    "                # Gradient bias - fc layer\n",
    "                dl_bias = copy(dl_sum)\n",
    "\n",
    "                # Gradient weight - fc layer\n",
    "                dl_weight = zeros(outputs.shape + flattens.shape, dtype=float64)\n",
    "                for i in range(outputs.shape[0]):\n",
    "                    dl_weight[i,:] = outputs[i] * flattens\n",
    "                dl_weight[label,:] = (outputs[label] - 1) * flattens\n",
    "\n",
    "                dl_flattens = zeros(flattens.shape, dtype=float64)\n",
    "                for j in range(flattens.shape[0]):\n",
    "                    dl_flattens[j] = sum(dl_sum * self.weight[:,j])\n",
    "\n",
    "                # Gradient output pooling layer 3\n",
    "                dl_pl3 = dl_flattens.reshape(pl3.shape)\n",
    "\n",
    "                # Gradient output convolution layer 3\n",
    "                dl_cl3 = zeros(cl3.shape, dtype=float64)\n",
    "                for nk in range(pl3.shape[0]):\n",
    "                    for row in range(pl3.shape[1]):\n",
    "                        for col in range(pl3.shape[2]):\n",
    "                            u_max, v_max = index_loc3[nk,row,col]\n",
    "                            dl_cl3[nk, u_max, v_max] = dl_pl3[nk,row,col]\n",
    "\n",
    "                # Gradient output convolution layer 3\n",
    "                dl_sum3 = dl_cl3 * d_cl3_sum3\n",
    "\n",
    "                # Gradient bias_3 - convolution layer 3\n",
    "                dl_bias_3 = zeros(self.kernel_3.shape[0], dtype=float64)\n",
    "\n",
    "                # Gradient kernel_3 - convolution layer 3\n",
    "                dl_kernel_3 = zeros(self.kernel_3.shape, dtype=float64)\n",
    "                for nkc in range(self.kernel_3.shape[0]):\n",
    "                    dl_bias_3[nkc] = sum(dl_sum3[nkc])\n",
    "                    for nkp in range(self.kernel_3.shape[1]):\n",
    "                        for row in range(self.kernel_3.shape[2]):\n",
    "                            for col in range(self.kernel_3.shape[3]):\n",
    "                                dl_kernel_3[nkc,nkp,row,col] = sum(dl_sum3[nkc] * pl2[nkp][row:(row+cl3.shape[1]), col:(col+cl3.shape[2])])\n",
    "\n",
    "                # Gradient output pooling layer 2\n",
    "                dl_pl2 = zeros(pl2.shape, dtype=float64)\n",
    "                for ni in range(pl2.shape[0]):\n",
    "                    for row in range(pl2.shape[1]):\n",
    "                        for col in range(pl2.shape[2]):\n",
    "                            dl_pl2[ni,row,col] = sum(dl_sum3 * d_sum3_pl2[ni,row,col])\n",
    "\n",
    "                # Gradient output convolution layer 2\n",
    "                dl_cl2 = zeros(cl2.shape, dtype=float64)\n",
    "                for nk in range(pl2.shape[0]):\n",
    "                    for row in range(pl2.shape[1]):\n",
    "                        for col in range(pl2.shape[2]):\n",
    "                            u_max, v_max = index_loc2[nk, row, col]\n",
    "                            dl_cl2[nk, u_max, v_max] = dl_pl2[nk,row,col]\n",
    "\n",
    "                # Gradient output convolution layer 2\n",
    "                dl_sum2 = dl_cl2 * d_cl2_sum2\n",
    "\n",
    "                # Gradient bias_2 - convolution layer 2\n",
    "                dl_bias_2 = zeros(self.kernel_2.shape[0], dtype=float64)\n",
    "\n",
    "                # Gradient kernel_2 - convolution layer 2\n",
    "                dl_kernel_2 = zeros(self.kernel_2.shape, dtype=float64)\n",
    "                for nkc in range(self.kernel_2.shape[0]):\n",
    "                    dl_bias_2[nkc] = sum(dl_sum2[nkc])\n",
    "                    for nkp in range(self.kernel_2.shape[1]):\n",
    "                        for row in range(self.kernel_2.shape[2]):\n",
    "                            for col in range(self.kernel_2.shape[3]):\n",
    "                                dl_kernel_2[nkc,nkp,row,col] = sum(dl_sum2[nkc] * pl1[nkp][row:(row+cl2.shape[1]), col:(col+cl2.shape[2])])\n",
    "\n",
    "                # Gradient output pooling layer 1\n",
    "                dl_pl1 = zeros(pl1.shape, dtype=float64)\n",
    "                for ni in range(pl1.shape[0]):\n",
    "                    for row in range(pl1.shape[1]):\n",
    "                        for col in range(pl1.shape[2]):\n",
    "                            dl_pl1[ni,row,col] = sum(dl_sum2 * d_sum2_pl1[ni,row,col])\n",
    "\n",
    "                # Gradient output convolution layer 1\n",
    "                dl_cl1 = zeros(cl1.shape, dtype=float64)\n",
    "                for nk in range(pl1.shape[0]):\n",
    "                    for row in range(pl1.shape[1]):\n",
    "                        for col in range(pl1.shape[2]):\n",
    "                            i_max, j_max = index_loc1[nk,row,col]\n",
    "                            dl_cl1[nk,i_max,j_max] = dl_pl1[nk,row,col]\n",
    "\n",
    "                dl_sum1 = dl_cl1 * d_cl1_sum1\n",
    "\n",
    "                # Gradient bias_1 - convolution layer 1\n",
    "                dl_bias_1 = zeros(self.kernel_1.shape[0], dtype=float64)\n",
    "\n",
    "                # gradient kernel_1 - convolution layer 1\n",
    "                dl_kernel_1 = zeros(self.kernel_1.shape, dtype=float64)\n",
    "                for nk in range(self.kernel_1.shape[0]):\n",
    "                    dl_bias_1[nk] = sum(dl_sum1[nk])\n",
    "                    for row in range(self.kernel_1.shape[1]):\n",
    "                        for col in range(self.kernel_1.shape[2]):\n",
    "                            dl_kernel_1[nk,row,col] = sum(dl_sum1[nk] * image[row:(row+cl1.shape[1]), col:(col+cl1.shape[2])])\n",
    "\n",
    "                # Update parameters: kernel, bias, weight\n",
    "                self.kernel_1 = self.kernel_1 - learn_rate * dl_kernel_1\n",
    "                self.bias_1 = self.bias_1 - learn_rate * dl_bias_1\n",
    "                self.kernel_2 = self.kernel_2 - learn_rate * dl_kernel_2\n",
    "                self.bias_2 = self.bias_2 - learn_rate * dl_bias_2\n",
    "                self.kernel_3 = self.kernel_3 - learn_rate * dl_kernel_3\n",
    "                self.bias_3 = self.bias_3 - learn_rate * dl_bias_3\n",
    "                self.weight = self.weight - learn_rate * dl_weight\n",
    "                self.bias = self.bias - learn_rate * dl_bias\n",
    "\n",
    "                progressbar_image.update()\n",
    "\n",
    "            progressbar_image.close()\n",
    "\n",
    "            train_epochs.append(epoch+1)\n",
    "\n",
    "            # Hitung loss\n",
    "            train_losses = array(train_losses)\n",
    "            train_average_losses.append(train_losses.mean())\n",
    "\n",
    "            # Hitung akurasi\n",
    "            accuracy = correct_labels/len(labels) * 100\n",
    "            train_accuracies.append(accuracy)\n",
    "\n",
    "            print(\"Epoch ke-{}: rata-rata loss {}, akurasi {:02.2f}%.\".format(epoch+1, train_losses.mean(), accuracy))\n",
    "\n",
    "        return (train_epochs, train_average_losses, train_accuracies)\n",
    "\n",
    "    def testing(self, images_testing, labels_testing):\n",
    "        \"\"\"\n",
    "        Fungsi untuk melakukan pengujian data.\n",
    "\n",
    "        :param images_testing: data citra uji.\n",
    "        :param labels_testing: data label dari citra uji.\n",
    "        :return: \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        len_perm = random.permutation(len(images_testing))\n",
    "\n",
    "        images = images_testing[len_perm]\n",
    "        labels = labels_testing[len_perm]\n",
    "\n",
    "        test_loss_values = []\n",
    "        test_average_loss = []\n",
    "        correct_labels = 0\n",
    "        output = []\n",
    "        result_class =[]\n",
    "\n",
    "        counter_img = 0\n",
    "\n",
    "        print(\"Pengujian data: \")\n",
    "\n",
    "        for image, label in zip(images, labels):\n",
    "            flattens, outputs = self.forwardpass(image)\n",
    "            test_loss_values.append(cross_entropy(outputs[label]))\n",
    "            # Menyimpan nilai outputs\n",
    "            output.append(outputs)\n",
    "\n",
    "            identify_image = argmax(outputs)\n",
    "            result_class.append(identify_image)\n",
    "\n",
    "            counter_img += 1\n",
    "            print(\"Data ke-{}: kelas sebenarnya {}, diidentifikasi sebagai kelas {}, dengan hasil {}.\".format(counter_img, label, identify_image , str(outputs)))\n",
    "\n",
    "        # Hitung loss\n",
    "        test_loss_values = array(test_loss_values)\n",
    "        test_average_loss.append(test_loss_values.mean())\n",
    "\n",
    "        saved_outputs = array(output)\n",
    "        actual_class = array(labels)\n",
    "        result_class = array(result_class)\n",
    "\n",
    "        print(\"Nilai rata-rata loss {}.\".format(test_loss_values.mean()))\n",
    "\n",
    "        return test_average_loss, saved_outputs, actual_class, result_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "d3MyHUloNjGpIgY4CpYJZo",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "numpy.set_printoptions(edgeitems=1000, linewidth=100000)\n",
    "\n",
    "convnet_d_model = ConvNet(6, 6, 6)\n",
    "\n",
    "# Pelatihan data\n",
    "mfid_images_training = load(\"data/train_mfid_images.npy\")\n",
    "images_training = (mfid_images_training/255)-0.5\n",
    "\n",
    "labels_training = load(\"data/train_mfid_labels.npy\")\n",
    "\n",
    "# Pengujian data\n",
    "mfid_image_testing = load(\"data/test_mfid_images.npy\")\n",
    "images_testing = (mfid_image_testing/255)-0.5\n",
    "labels_testing = load(\"data/test_mfid_labels.npy\")\n",
    "\n",
    "# Pelatihan model\n",
    "train_d_model = convnet_d_model.training(images_training, labels_training, 15, 0.005)\n",
    "train_epochs = train_d_model[0]\n",
    "train_average_losses = train_d_model[1]\n",
    "train_accuracies = train_d_model[2]\n",
    "\n",
    "# Pengujian model\n",
    "test_d_model = convnet_d_model.testing(images_testing, labels_testing)\n",
    "test_average_loss = test_d_model[0]\n",
    "outputs = test_d_model[1]\n",
    "actual_class = test_d_model[2]\n",
    "result_class = test_d_model[3]\n",
    "\n",
    "# Simpan model setelah dilatih\n",
    "with open(\"trained-model/d_model.bin\", \"wb\") as file:\n",
    "    pickle.dump(convnet_d_model, file)\n",
    "\n",
    "# Plot diagram nilai loss\n",
    "figure = plot.figure()\n",
    "plot.subplots_adjust(hspace=0.5)\n",
    "\n",
    "loss_graph = figure.add_subplot(2, 1, 1, ylabel=\"Train loss\", xlabel=\"Epoch\")\n",
    "loss_graph.plot(train_epochs, train_average_losses, label=\"Average loss\", color=\"red\")\n",
    "loss_graph.legend(loc=\"center\")\n",
    "\n",
    "# Diagram akurasi selama pelatihan data\n",
    "accuracy_graph = figure.add_subplot(2, 1, 2, ylabel=\"Train accuracy\", xlabel=\"Epoch\")\n",
    "accuracy_graph.plot(train_epochs, train_accuracies, label=\"Accuracy\", color=\"green\")\n",
    "accuracy_graph.legend(loc=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DQU0kfVFv8VhnxocATcIvq",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "CM_ModD = ConfusionMatrix(actual_class, result_class)\n",
    "\n",
    "CM_ModD.display_table()\n",
    "\n",
    "print(\"Accuracy: {:02.8f}\".format(CM_ModD.accuracy()))\n",
    "print(\"Precision: {:02.8f}\".format(CM_ModD.precision()))\n",
    "print(\"Recall: {:02.8f}\".format(CM_ModD.recall()))\n",
    "print(\"F1-Score: {:02.8f}\".format(CM_ModD.f1_score()))\n",
    "\n",
    "CM_ModD.model_performance_graph()"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "computation_mode": "JUPYTER",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
